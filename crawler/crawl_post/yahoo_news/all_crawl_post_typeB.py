import csv
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin
from datetime import datetime, timedelta, timezone
import json
from summa import summarizer, keywords


# ê²€ìƒ‰í•  ê¸°ì—… ë¦¬ìŠ¤íŠ¸
COMPANIES = ["nvda", "tsla", "aapl", "meta", "msft", "amzn", "goog"]
BASE_URL_COMPANY = "https://finance.yahoo.com/quote/{}/latest-news/"
BASE_URL_MARKET = "https://finance.yahoo.com/topic/stock-market-news/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}
API_URL = "http://localhost:3000/api/news"

def fetch_news_page(url):
    """Yahoo Financeì—ì„œ ë‰´ìŠ¤ í˜ì´ì§€ ìš”ì²­"""
    print(f"ğŸ“¡ í¬ë¡¤ë§ ì¤‘: {url}")
    
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"âš ï¸ ë‰´ìŠ¤ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def parse_news_articles(html, company=None):
    """ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì—ì„œ ì œëª©ê³¼ URL ì¶”ì¶œ, ê¸°ì—… ì •ë³´ í¬í•¨"""
    soup = BeautifulSoup(html, "html.parser")
    articles = soup.select("div.content.yf-82qtw3")

    news_list = []
    for article in articles:
        title_tag = article.select_one("h3.clamp.yf-82qtw3")
        title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

        article_link_tag = article.select_one("a.subtle-link")
        url = urljoin("https://finance.yahoo.com", article_link_tag["href"]) if article_link_tag else None
        
        news_list.append({
            "title": title,
            "link": url,
            "company": company  # ê¸°ì—… ì •ë³´ ì¶”ê°€
        })
    
    return news_list

def fetch_article_content(url):
    """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§"""
    if not url:
        return "ë³¸ë¬¸ ì—†ìŒ", None
    
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"âš ï¸ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {url} - {e}")
        return "ë³¸ë¬¸ ì—†ìŒ", None
    
    soup = BeautifulSoup(response.text, "html.parser")
    time_tag = soup.select_one("time.byline-attr-meta-time")
    datetime_str = None
    if time_tag and time_tag.has_attr("datetime"):
        datetime_str = time_tag["datetime"]
        datetime_str = datetime.fromisoformat(datetime_str.replace("Z", "+00:00")).date()
        print(f"ğŸ•’ ê¸°ì‚¬ ê²Œì‹œì¼: {datetime_str}")
    content_div = soup.select_one("div.atoms-wrapper")
    
    if content_div:
        paragraphs = content_div.find_all("p")
        content = "\n".join([p.text.strip() for p in paragraphs if p.text.strip()])
        return content, datetime_str
    else:
        return "ë³¸ë¬¸ ì—†ìŒ", None
    
def summa_summarize(article):
    summary_content = summarizer.summarize(article, ratio=0.5)
    key_words = keywords.keywords(article, ratio=0.1)
    return {
        "summary": summary_content.strip(),
        "keywords": key_words.replace('\n', ', ')
    }

def post_data_to_server(item):
    headers = {"Content-Type": "application/json"}
    json_data = json.dumps([item], ensure_ascii=False)  # Wrap in list

    response = requests.post(API_URL, data=json_data, headers=headers)
    print(f"ğŸ“¡ POST {API_URL} - Status: {response.status_code}, Response: {response.text}")

def main(start_date):
    start_date = datetime.fromisoformat(start_date).date()
    for company in COMPANIES:
        html = fetch_news_page(BASE_URL_COMPANY.format(company))
        if not html:
            continue

        articles = parse_news_articles(html, company)
        for article in articles:

            article_content, article_date = fetch_article_content(article["link"])
            summary = summa_summarize(article_content)
            content_to_post = summary["summary"] if summary["summary"] else article_content
                
            item = {
                "date": article_date.isoformat() if article_date else None,  # ê²Œì‹œì¼ì„ ë¬¸ìì—´ í˜•íƒœë¡œ ì§ë ¬í™”
                "title": article["title"],
                "link": article["link"] or "URL ì—†ìŒ",
                "content": content_to_post,
                "company": article["company"] or "general_market",
                "keywords": summary["keywords"],
            }
            post_data_to_server(item)
            print(f"ğŸ“° ì €ì¥ëœ ê¸°ì‚¬ ì œëª©: {article['title']} (ê²Œì‹œì¼: {article_date})")

            time.sleep(2.5)  # ìš”ì²­ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ëŒ€ê¸°

    # ì „ì²´ ì£¼ì‹ ì‹œì¥ ë‰´ìŠ¤ í¬ë¡¤ë§ (ê¸°ì—… ì •ë³´ ì—†ìŒ)
    html = fetch_news_page(BASE_URL_MARKET)
    if html:
        articles = parse_news_articles(html, company=None)
        for article in articles:

            article_content, article_date = fetch_article_content(article["link"])
            summary = summa_summarize(article_content)
            content_to_post = summary["summary"] if summary["summary"] else article_content

            item = {
                "date": article_date.isoformat() if article_date else None,  # ê²Œì‹œì¼ì„ ë¬¸ìì—´ í˜•íƒœë¡œ ì§ë ¬í™”
                "title": article["title"],
                "link": article["link"] or "URL ì—†ìŒ",
                "content": content_to_post,
                "company": "general_market",  # ê¸°ì—… ì •ë³´ ì—†ìŒ (ì „ì²´ ì‹œì¥ ë‰´ìŠ¤)
                "keywords": summary["keywords"],
            }
            post_data_to_server(item)
            print(f"ğŸ“° ì €ì¥ëœ ê¸°ì‚¬ ì œëª©: {article['title']} (ê²Œì‹œì¼: {article_date})")
            if summary:
                key_words = keywords.keywords(content_to_post, ratio=0.1)
                print("ğŸ”‘ í‚¤ì›Œë“œ:", key_words.replace('\n', ', '))

if __name__ == "__main__":
    current = datetime(2023, 11, 1)
    today = datetime.now().date()

    while current.date() <= today:
        main(current.date().isoformat())
        current += timedelta(days=1)

'''
nohup python api/crawl_post/all_crawl_post_typeB.py > logs.txt 2>&1 &
ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† run ë˜ë„ë¡.
'''
